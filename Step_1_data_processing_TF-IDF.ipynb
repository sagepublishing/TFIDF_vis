{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a variable word_data as input.  This is a list of text documents. Then tokenize, stem and TF-IDF vecorize.  Output should be a set of TF-IDF vectors for each DOI.\n",
    "\n",
    "This all takes place in-memory.  It may be that this needs edited if we want to grow the dataset to a larger size.  For now, it should give better speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Step 1:  Calculating TF-IDF data\n",
      "2018-02-16 13:16:36.531119\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "print('Step 1:  Calculating TF-IDF data')\n",
    "from datetime import datetime as dt\n",
    "t_start = dt.now()\n",
    "print(t_start)\n",
    "print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import configuration\n",
    "Using a separate file config.py to create shared variables for the various notebooks in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from config import Config as c\n",
    "\n",
    "# inputs\n",
    "\n",
    "years = c.years\n",
    "\n",
    "\n",
    "word_datapath = c.word_datapath\n",
    "tfidf_datapath = c.tfidf_datapath\n",
    "labels_path = c.labels_path\n",
    "\n",
    "vectorizer_datapath = c.vectorizer_datapath\n",
    "filepaths_pkl = c.filepaths_pkl\n",
    "\n",
    "# outputs\n",
    "dois_pkl = c.dois_pkl\n",
    "all_dois_pkl = c.all_dois_pkl\n",
    "working_data = c.working_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def strip_stem(s): # removes punctuation and stems words\n",
    "    s = tokenizer.tokenize(s)\n",
    "    return ' '.join([stemmer.stem(word) for word in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load word data and corresponding doi_ls\n",
    "with open(word_datapath, 'rb') as f:\n",
    "    word_data = pickle.load(f)\n",
    "with open(dois_pkl, 'rb') as f:\n",
    "    dois = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the template dataframe for that will eventually take the data for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7710, 11)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "data = pd.read_csv(working_data, index_col=0)\n",
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFINE DOI_LS VARIABLE HERE\n",
    "dois = list(data['DI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7710, 7710)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dois), len(word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame initialised\n"
     ]
    }
   ],
   "source": [
    "# write to file\n",
    "data.to_csv(working_data)\n",
    "print('DataFrame initialised')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TF-IDF indexing.\n"
     ]
    }
   ],
   "source": [
    "print('Starting TF-IDF indexing.')\n",
    "# stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "my_words = ['et','al','use','article','introduction','abstract','title', 'nan'] # note that there are 'NaN's in WoS data!\n",
    "\n",
    "# Add custom stopwords here.  E.g. ['sensor','network','data','node']  are so common in DSN\n",
    "# that they appear in almost every paper and make it hard to differentiate between clusters.  \n",
    "custom_stops = ['4th']\n",
    "my_words = my_words+custom_stops\n",
    "\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(my_words)\n",
    "my_stop_words = set(my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix: (7710, 84)\n",
      "I.e. 7710 documents with 84 unique words in them\n"
     ]
    }
   ],
   "source": [
    "# tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=my_stop_words,\n",
    "                             max_df = 0.99, # ignore the most frequent 1% of words\n",
    "                            min_df = 2, # ignore words that appear in this many docs or fewer\n",
    "                            ngram_range =(1,2)) # improves model for small datasets, but causes slow-down\n",
    "tfidf = vectorizer.fit_transform(word_data)\n",
    "shp = np.shape(tfidf)\n",
    "print('Shape of TF-IDF matrix:', shp)\n",
    "print('I.e. {} documents with {} unique words in them'.format(shp[0],shp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF indexing complete. Writing to file.\n"
     ]
    }
   ],
   "source": [
    "# cosine distances\n",
    "sim = tfidf*tfidf.T\n",
    "# cosine_sims = sim.todense() # this can be memory-hungry and isn't needed for the visualisation.\n",
    "print('TF-IDF indexing complete. Writing to file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(tfidf, open(tfidf_datapath,'wb'))\n",
    "# pickle.dump(cosine_sims, open(cosine_sims_datapath,'wb'))\n",
    "pickle.dump(vectorizer, open(vectorizer_datapath,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 completed in  0:00:17.742964\n"
     ]
    }
   ],
   "source": [
    "print('Step 1 completed in ', dt.now()-t_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
