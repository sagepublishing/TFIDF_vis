{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Consider hierarchical methods. Since these should be able to extract concepts better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Step 4:  K Means \n",
      "Starting at 2018-02-16 20:55:48.071161\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "print('Step 4:  K Means ')\n",
    "from datetime import datetime as dt\n",
    "t_start = dt.now()\n",
    "print('Starting at', t_start)\n",
    "print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure\n",
    "from config import Config as c\n",
    "# set K parameter here:\n",
    "n_clusters = c.n_clusters\n",
    "# which cluster set to use?\n",
    "# ['tsne', 'raw'] \n",
    "cluster_set = 'tsne'\n",
    "\n",
    "\n",
    "\n",
    "# other global variables:\n",
    "doi_datapath = c.dois_pkl\n",
    "word_datapath = c.word_datapath\n",
    "tfidf_datapath = c.tfidf_datapath\n",
    "vectorizer_datapath = c.vectorizer_datapath\n",
    "working_data =  c.working_data\n",
    "\n",
    "# output paths\n",
    "data_out = c.working_data\n",
    "data_out_xl = c.kmeans_out_xl\n",
    "data_out_csv = c.kmeans_out_csv\n",
    "\n",
    "# load vector data\n",
    "tfidf = pickle.load(open(tfidf_datapath,'rb'))\n",
    "vectorizer = pickle.load(open(vectorizer_datapath,'rb'))\n",
    "\n",
    "# global variable for ordering\n",
    "dois = pickle.load(open(doi_datapath,'rb'))\n",
    "data = pd.read_csv(working_data,index_col = 0)\n",
    "cites = list(data.Citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.99, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words={'this', 'before', 'among', 'only', 'any', 'eg', 'mine', 'otherwise', 'they', 'introduction', 'where', 'ltd', 'until', 'had', 'yourself', 'therefore', 'then', 'sometime', 'side', 'whereafter', 'myself', 'nine', 'even', 'etc', 'thereafter', 'will', 'name', 'some', 'front', 'mill', 'article...same', 'in', 'hereupon', 'at', 'while', 'now', 'nothing', 'beyond', 'must', 'is', 'two', 'anywhere'},\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering AFTER dimensional reduction\n",
    "1. get which articles are in which cluster\n",
    "2. sum the tfidf for the articles in each group\n",
    "3. argsort the sum for each cluster\n",
    "4. use the argsort to pick keywords from vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-517925b47268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# cluster t-sne data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtsne_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TSNE1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'TSNE2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtsne_kmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtsne_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtsne_centers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsne_kmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m \u001b[1;31m# centers of each cluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtsne_klabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsne_kmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m \u001b[1;31m# cluster numbers for each paper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    885\u001b[0m         \"\"\"\n\u001b[0;32m    886\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 887\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_check_fit_data\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    856\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_fit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[1;34m\"\"\"Verify that the number of samples given is larger than k\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    859\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             raise ValueError(\"n_samples=%d should be >= n_clusters=%d\" % (\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 44\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# cluster t-sne data\n",
    "tsne_data = np.matrix(data[['TSNE1','TSNE2']])\n",
    "tsne_kmeans = KMeans(n_clusters=n_clusters, ).fit(tsne_data) \n",
    "tsne_centers = tsne_kmeans.cluster_centers_ # centers of each cluster\n",
    "tsne_klabels = tsne_kmeans.labels_ # cluster numbers for each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_klabels, np.shape(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work out the most significant words in each cluster and add them as cluster keywords so that we know what each cluster is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(tsne_data),np.shape(tsne_centers), tsne_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {}\n",
    "mat = tfidf.todense() # this step kills it if dataset too large\n",
    "i = 0\n",
    "for label in tsne_klabels: # labels are cluster numbers\n",
    "    if label in dct:\n",
    "        # add together all rows with the same cluster number\n",
    "        dct[label] = dct[label] + mat[i,:]\n",
    "    else:\n",
    "        # initialise if there's nothing to add to\n",
    "        dct[label] = mat[i,:]\n",
    "    i+=1\n",
    "    \n",
    "dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_terms = vectorizer.get_feature_names() # gets the words from the vocabulary\n",
    "tsne_terms_df =[]\n",
    "for i in range(n_clusters):\n",
    "    ls_ = []\n",
    "    # get the indexes of the top 10 terms in each cluster\n",
    "    indexes = list(np.array(dct[i].argsort()[:, ::-1][0])[0])[:10] # this conversion is not very neat.\n",
    "#     print(indexes)\n",
    "    for ind in indexes:\n",
    "        ls_.append(tsne_terms[int(ind)]) \n",
    "    tsne_terms_df.append(ls_[:3])\n",
    "    \n",
    "# show first 3 cluster labels\n",
    "tsne_terms_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quick fix\n",
    "klabels = tsne_klabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dois_ls = []\n",
    "for i in range(n_clusters):\n",
    "    # indices of papers that fall within ith cluster\n",
    "    indices = [j for j, x in enumerate(klabels) if x ==i]\n",
    "    # print(indices)\n",
    "    doi_ls = [dois[j][:7] + '/' + dois[j][8:] \n",
    "             for j in indices]\n",
    "    dois_ls.append(doi_ls)\n",
    "\n",
    "# len(links_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose which clusters to add to your dataframe\n",
    "This is not the neatest solution to this.  Do something better.\n",
    "\n",
    "This part is a placeholder for later.  You potentially get better results by using higher dimensionality data, so KMeans on that data might be a worthwhile option.  Note that it is very slow and makes the final visualisation look scruffy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if cluster_set == 'tsne':\n",
    "    terms_df = tsne_terms_df\n",
    "    centers =  tsne_centers\n",
    "    klabels = tsne_klabels\n",
    "    kmeans = tsne_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best cited clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## try dropping the top x percentile of papers \n",
    "\n",
    "nz_cites_ls = []\n",
    "mean_cites_ls =[]\n",
    "cites_ls_ls = []\n",
    "mean_outlier_ls = []\n",
    "for j in range(n_clusters):\n",
    "    # get the index for each paper in jth cluster\n",
    "    cluster_indices = [i for i, x in enumerate(klabels) if x ==j]\n",
    "    # get the citations for those indices\n",
    "    cluster_cites = [cites[i] \n",
    "                    if np.isnan(cites[i])==False \n",
    "                    else 0\n",
    "                    for i in cluster_indices] # switching NaNs to zeros in order to ensure following calculations work\n",
    "    \n",
    "    # count non-zero cites for each cluster\n",
    "    nz_cites = float(np.count_nonzero(np.array(cluster_cites)))#/len(cluster_cites))\n",
    "    nz_cites_ls.append(nz_cites) # list of nz cite counts for all clusters \n",
    "    \n",
    "    # exclude outliers\n",
    "    # ptl80 = np.percentile(cluster_cites, 80)\n",
    "#     n_outliers = 2\n",
    "#     cluster_exc_max = [cites[i]\n",
    "#                     if np.isnan(cites[i])==False \n",
    "#                     else 0\n",
    "#                     for i in cluster_indices] # switching NaNs to zeros in order to ensure following calculations work\n",
    "#     for i in range(n_outliers):\n",
    "#         max_cite = max(cluster_exc_max)\n",
    "#         cluster_exc_max.remove(max_cite) \n",
    "#     mean_outlier = np.mean(cluster_exc_max)\n",
    "#     mean_outlier_ls.append(mean_outlier)\n",
    "                     \n",
    "    # work out mean citations of each cluster\n",
    "    mean_cites = np.mean(np.array(cluster_cites))\n",
    "    mean_cites_ls.append(mean_cites)\n",
    "    \n",
    "    # add the list of citations for each item in the cluster\n",
    "    cites_ls_ls.append(cluster_cites)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe at a cluster level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Cluster': terms_df,\n",
    "    'nz_cites': nz_cites_ls,\n",
    "    'mean_cites':mean_cites_ls,\n",
    "    'cites':cites_ls_ls,\n",
    "#     'mean_outlier':mean_outlier_ls,  \n",
    "    'dois_ls':dois_ls\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['len_cites'] = [len(i) for i in df['cites']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.sort_values('mean_outlier', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Cluster_no'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['nz_pc'] = df['nz_cites']/df['len_cites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = df['nz_cites'].hist(alpha=0.3)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "s = df['nz_pc'].hist(alpha=0.3)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# p = df['mean_outlier'].hist(alpha=0.3)\n",
    "# q = df['mean_cites'].hist(alpha=0.3)\n",
    "# p,q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dois[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_cluster_dict = {}\n",
    "doi_clusterno_dict = {}\n",
    "for index,row in df.iterrows():\n",
    "    doi_ls = row.dois_ls\n",
    "    for doi in doi_ls:\n",
    "        doi_cluster_dict[doi] = row.Cluster\n",
    "        doi_clusterno_dict[doi] = row.Cluster_no\n",
    "\n",
    "cluster_ls = [str(\n",
    "    doi_cluster_dict[\n",
    "        doi[:7]+'/'+doi[8:]\n",
    "    ]) for doi in dois]\n",
    "\n",
    "clusterno_ls = [doi_clusterno_dict[doi[:7]+'/'+doi[8:]] for doi in dois]\n",
    "\n",
    "len(clusterno_ls),len(cluster_ls), len(dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_kws_ls= []\n",
    "\n",
    "for row in mat:\n",
    "    indexes = list(np.array(row.argsort()[:, ::-1][0])[0])[:10]\n",
    "    article_kws_ls.append([tsne_terms[ind] for ind in indexes][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_kws_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cluster'] = cluster_ls\n",
    "data['Cluster_no'] = clusterno_ls\n",
    "data['Article_kws'] = article_kws_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(data_out)\n",
    "df.to_excel(data_out_xl)\n",
    "df.to_csv(data_out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = str(dt.now()-t_start)\n",
    "print('Done in '+s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
